{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M1_AST_09_Scalable_Programming_using_PySpark_C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShailendraSSY/BigData/blob/main/M1_AST_09_Scalable_Programming_using_PySpark_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps9llghv8jX1"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 9: PySpark Transformation on Paired RDD's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeP1PAXf8jYD"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkwaW3k58jYG"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* perform paired RDD (Resilient Distributed Datasets) operations like transformations.\n",
        "\n",
        "* know the ways to transform the paired RDD's with by applying tranformations and action on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qPzwUkKjntI"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfx-uQpTjpgB"
      },
      "source": [
        "### Paired RDD's\n",
        "Pair-wise RDD's are RDD's in which each element is in the form of tuples where the first element is key and the second element is the value.\n",
        "* They have the keys and values associated. \n",
        "* Here keys are not distinct and a single value is assigned for a value.\n",
        "* They are used to perform aggregate operations. \n",
        "* Paired RDD's exposes additional transformation and actions.\n",
        "* They are derived from base RDD. \n",
        "* It supports all base operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZfXZMpmdJ8p"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2200023\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HM6RuGGdJ8-"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9910549998\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "355936f4-8466-4b47-9fa1-d0d12fd78c75"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M1_AST_09_Scalable_Programming_using_PySpark_C\" #name of the notebook\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2200023&recordId=2274\"></script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q9PCT0kBVB3"
      },
      "source": [
        " **Install PySpark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xzay908G5qQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4feb681f-5a48-48a5-dae9-8ecd15769d63"
      },
      "source": [
        "!pip -q install pyspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 72 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 43.8 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BobYAePRT6Ok"
      },
      "source": [
        "**Creating Spark Session**\n",
        "\n",
        "Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0 (Instead of having various contexts, everything is encapsulated in a Spark session)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-tj32cQHmBb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "801edc6a-7fa0-4377-d545-c7a322b75416"
      },
      "source": [
        "# Start spark session\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf  # User Defined Functions\n",
        "from pyspark.sql.types import StringType\n",
        "from operator import add\n",
        "spark = SparkSession.builder.appName('Paired_RDD').getOrCreate()\n",
        "spark"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c8281864add9:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Paired_RDD</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f16676d5e90>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypfIQnQczPMy"
      },
      "source": [
        "# Accessing sparkContext from sparkSession instance.\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOMczmP4j4da"
      },
      "source": [
        "### Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD6U16Q7hfly"
      },
      "source": [
        "Transformation is a function that produces new RDD from the existing RDDs. It takes RDD as input and produces one or more RDD as output.\n",
        "\n",
        "Example:- map, filter, flatMap, groupByKey, reduceByKey, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLXxvk4ISneY"
      },
      "source": [
        "**map():** Map transformation is applied over each and every function of an RDD / Data Frame. The return type is a new RDD or dataframe where the Map function is applied. It is used to apply operations over every element."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue_fKF3QS4oE"
      },
      "source": [
        "1.Perform square of a number by map transformation for the below defined data.\n",
        "\n",
        "data = [ 1 , 2 , 3 , 4 , 5 , 6 , 7] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4gsga2MTMUH"
      },
      "source": [
        "# Defining the data\n",
        "data = [ 1 , 2 , 3 , 4 , 5 , 6 , 7] \n",
        "rdd = sc.parallelize(data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxXx5jcBTb_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad67a4e-c380-45ac-dc58-8d3b18c24725"
      },
      "source": [
        "# Finding the squares by using the map \n",
        "rdd1 = rdd.map(lambda x : [x*x])\n",
        "rdd1.collect()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1], [4], [9], [16], [25], [36], [49]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrMFnwN4Wo9x"
      },
      "source": [
        "**flatMap():**  It applies function to the elements and returns only a single RDD and it will not return a nested one. The most simple use of flatMap() is to split each input string into words. The key difference between map() and flatMap() is that map() returns only one element, while flatMap() can return a list of elements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gft4RaHzWzRy"
      },
      "source": [
        "1. Apply the flatMap transformation on the below defined data and check how the data is being transformed.\n",
        "\n",
        "  data = [ 1 , 2 , 3 , 4 , 5 , 6 , 7] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wd_L6_2GLKZ"
      },
      "source": [
        "# Defining the data\n",
        "data = [ 1 , 2 , 3 , 4 , 5 , 6 , 7] \n",
        "rdd = sc.parallelize(data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dLvLkRcY9hD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b46aac58-2269-4c45-b952-8cb3d3793e58"
      },
      "source": [
        "# Finding the squares by using the flatmap \n",
        "rdd.flatMap(lambda x : [x*x]).collect()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 9, 16, 25, 36, 49]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_MSzo4vON1i"
      },
      "source": [
        "**Union Operation:** Union is used to append two dataframes with same schema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1XKJkzCOlpf"
      },
      "source": [
        "1. Use union operation for two RDD's given below:\n",
        "  \n",
        "  data1 = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9]\n",
        "\n",
        "  data2 = [ 2 , 4 , 5 , 7 , 9 ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXJcedZsOCC_"
      },
      "source": [
        "# Defining the data\n",
        "rdd1 = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "rdd2 = sc.parallelize([ 2, 4, 5, 7, 9])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inXoOkB9OIVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352c57e3-4126-49a2-edd5-9c7dabf1fffd"
      },
      "source": [
        "# Using union operator\n",
        "rdd3 = rdd1.union(rdd2)\n",
        "rdd3.collect()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 4, 5, 7, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8b9dgehoF0H"
      },
      "source": [
        "2. Use union operator and merge the below given data.\n",
        "\n",
        "  data  = [ \" mango \" , \" banana \" , \" apple \"]\n",
        "  \n",
        "  data2 = [ \" orange \" , \" grapes \" , \" pineapple \" , \" mango \" , \" banana \"]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUFHTl2fosGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716de656-a727-4f29-9872-59e198a89201"
      },
      "source": [
        "# Defining the data\n",
        "rdd1 = sc.parallelize([ \"mango\", \"banana\", \"apple\"])\n",
        "rdd2 = sc.parallelize([\"orange\", \"grapes\", \"pineapple\", \"mango\", \"banana\"])\n",
        "# Using union operator\n",
        "rdd3 = rdd1.union(rdd2)\n",
        "rdd3.collect()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mango',\n",
              " 'banana',\n",
              " 'apple',\n",
              " 'orange',\n",
              " 'grapes',\n",
              " 'pineapple',\n",
              " 'mango',\n",
              " 'banana']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnQ3lXzIdpB2"
      },
      "source": [
        "**Intersection:-** Using Intersection, we get only the common element of both the RDD's in a new RDD. The key rule of this function is that the two RDDs should be of the same type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6G68AmGdx-s"
      },
      "source": [
        "1. For the given below data find the common terms by using the intersection transformation.\n",
        "\n",
        "  data1 = [1, 10, 2, 3, 4, 5]\n",
        "\n",
        "  data2 = [1, 6, 2, 3, 7, 8]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arYpEhAbd6bu"
      },
      "source": [
        "# Defining the data\n",
        "rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
        "rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPtX2ckYeLLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e20699-8417-4b1a-e061-48d3161d7f72"
      },
      "source": [
        "rdd1.intersection(rdd2).collect()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHAxh8ssn6x4"
      },
      "source": [
        "2. For the given below data find the common terms by using the intersection transformation.\n",
        "\n",
        "  data = [ \" mango \" , \" banana \" , \" apple \" ] \n",
        "\n",
        "  data2 = [ \" orange \" , \" grapes \" , \" pineapple \" , \" mango \" , \" banana \" ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u2s_DW6pLKJ"
      },
      "source": [
        "# Defining the data\n",
        "rdd1 = sc.parallelize([ \"mango\" ,\"banana\", \"apple\" ])\n",
        "rdd2 = sc.parallelize([ \"orange\", \"grapes\", \"pineapple\", \"mango\", \"banana\"])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSaKKkhWpQgD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "713c2510-98ff-4334-f1a9-2bb7cee81a20"
      },
      "source": [
        "rdd1.intersection(rdd2).collect()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mango', 'banana']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYa3UkF7P50F"
      },
      "source": [
        "**filter():** filter() function returns a new RDD containing only the elements that meet a predicate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnWWc9H-bnys"
      },
      "source": [
        "1. Apply filter operation to the data defined below to find the perfect squares.\n",
        "\n",
        "  data1 = [1, 2, 3, 4, 7, 9, 12, 16, 21, 24, 25, 27, 30, 32 , 36, 44, 47, 49]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq2zHRWkcc0h"
      },
      "source": [
        "rdd = sc.parallelize([1, 2, 3, 4, 7, 9, 12, 16, 21, 24, 25, 27, 30, 32 , 36, 44, 47, 49])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G0ib7BDc2Et",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d84c654f-26b4-499f-87f1-97e6d5b99dd6"
      },
      "source": [
        "# Finding the perfect squares\n",
        "def is_perfect_square(n):\n",
        "        return round(n ** 0.5) ** 2 == n\n",
        "rdd.filter(is_perfect_square).collect()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 9, 16, 25, 36, 49]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GQwBWq_hT9l"
      },
      "source": [
        "**groupByKey():** When we use groupByKey() on a Paired RDD of (K, V) pairs, the data is shuffled according to the key value K."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJhoyVDBh4EB"
      },
      "source": [
        "1. Apply groupbykey operation to the data defined below.\n",
        "\n",
        "  data = [ ( \" banana \" ,  1 ) , ( \" apple \", 1 ), ( \"carrot \" , 1 ) , ( \" banana \" , 3 ) ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axrFoqy_hPPI"
      },
      "source": [
        "x = sc.parallelize([(\"banana\", 1), (\"apple\", 1), (\"carrot\", 1),(\"banana\",3)])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VEnu1xvh5un",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f28d366-2952-4f7c-816b-7bd551de86e0"
      },
      "source": [
        "x.groupByKey().map(lambda x : (x[0], list(x[1]))).collect()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('banana', [1, 3]), ('carrot', [1]), ('apple', [1])]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttCo7cGHhwmU"
      },
      "source": [
        "**reduceByKey:** When we use reduceByKey on a Paired RDD (K, V), the pairs on the same machine with the same key are combined, before the data is shuffled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIT9tzw2iOq5"
      },
      "source": [
        "1. Apply reducebykey operation to the data defined below.\n",
        "\n",
        "  data = [ ( \" banana \" , 1), ( \" apple \" , 1), ( \" carrot \" , 1), ( \" banana \" , 3 ) ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cAI0dB6iZcZ"
      },
      "source": [
        "x = sc.parallelize([(\"banana\", 1), (\"apple\", 1), (\"carrot\", 1),(\"banana\",3)])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLHLgkqribmv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8f0c9e5-5624-4de5-d86d-0cc47f742217"
      },
      "source": [
        "from operator import add\n",
        "x.reduceByKey(add).collect()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('banana', 4), ('carrot', 1), ('apple', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY_AkPX9wJ-L"
      },
      "source": [
        "**combineByKey:** It  is very similar to combiner in Hadoop MapReduce programming. Internally, the combineByKey function efficiently combines the values of a PairRDD partition by applying aggregation function. The main objective of combineByKey transformation is transforming any $PairRDD[(K,V)]$ to the $RDD[(K,C)]$ where $C$ is the result of any aggregation of all values under key $K$.\n",
        "\n",
        "Spark combineByKey function uses following three functions as an argument,\n",
        "\n",
        "* createCombiner\n",
        "* mergeValue\n",
        "* mergeCombiners"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtj8PrmPxCiC"
      },
      "source": [
        "1. Find the average of the scores that are scored by each student by using combineByKey transformation. The student marks data is defined below.\n",
        "\n",
        "      student_rdd = sc.parallelize([\n",
        "  (\"ram\", \"Maths\", 83), (\"ram\", \"Physics\", 74), (\"ram\", \"Chemistry\", 91), (\"ram\", \"Biology\", 82), \n",
        "  \n",
        "  (\"suresh\", \"Maths\", 69), (\"suresh\", \"Physics\", 62), (\"suresh\", \"Chemistry\", 97), (\"suresh\", \"Biology\", 80), \n",
        "  \n",
        "  (\"ramesh\", \"Maths\", 78), (\"ramesh\", \"Physics\", 73), (\"ramesh\", \"Chemistry\", 68), (\"ramesh\", \"Biology\", 87), \n",
        "  \n",
        "  (\"john\", \"Maths\", 87), (\"john\", \"Physics\", 93), (\"john\", \"Chemistry\", 91), (\"john\", \"Biology\", 74), \n",
        "  \n",
        "  (\"sita\", \"Maths\", 56), (\"sita\", \"Physics\", 65), (\"sita\", \"Chemistry\", 71), (\"sita\", \"Biology\", 68), \n",
        "  \n",
        "  (\"harsha\", \"Maths\", 86), (\"harsha\", \"Physics\", 62), (\"harsha\", \"Chemistry\", 75), (\"harsha\", \"Biology\", 83), \n",
        "  \n",
        "  (\"Anu\", \"Maths\", 63), (\"Anu\", \"Physics\", 69), (\"Anu\", \"Chemistry\", 64),   (\"Anu\", \"Biology\", 60)], 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw0bWtkHxB9I"
      },
      "source": [
        "# Student marks \n",
        "student_rdd = sc.parallelize([\n",
        "  (\"ram\", \"Maths\", 83), (\"ram\", \"Physics\", 74), (\"ram\", \"Chemistry\", 91), (\"ram\", \"Biology\", 82), \n",
        "  (\"suresh\", \"Maths\", 69), (\"suresh\", \"Physics\", 62), (\"suresh\", \"Chemistry\", 97), (\"suresh\", \"Biology\", 80), \n",
        "  (\"ramesh\", \"Maths\", 78), (\"ramesh\", \"Physics\", 73), (\"ramesh\", \"Chemistry\", 68), (\"ramesh\", \"Biology\", 87), \n",
        "  (\"john\", \"Maths\", 87), (\"john\", \"Physics\", 93), (\"john\", \"Chemistry\", 91), (\"john\", \"Biology\", 74), \n",
        "  (\"sita\", \"Maths\", 56), (\"sita\", \"Physics\", 65), (\"sita\", \"Chemistry\", 71), (\"sita\", \"Biology\", 68), \n",
        "  (\"harsha\", \"Maths\", 86), (\"harsha\", \"Physics\", 62), (\"harsha\", \"Chemistry\", 75), (\"harsha\", \"Biology\", 83), \n",
        "  (\"Anu\", \"Maths\", 63), (\"Anu\", \"Physics\", 69), (\"Anu\", \"Chemistry\", 64), (\"Anu\", \"Biology\", 60)], 3)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g4ldVsyx7cN"
      },
      "source": [
        "# Defining createCombiner, mergeValue and mergeCombiner functions\n",
        "def createCombiner(tpl):\n",
        "    return (tpl[1], 1)\n",
        "    \n",
        "def mergeValue(accumulator, element): \n",
        "    return (accumulator[0] + element[1], accumulator[1] + 1)\n",
        "    \n",
        "def mergeCombiner(accumulator1, accumulator2): \n",
        "    return (accumulator1[0] + accumulator2[0], accumulator1[1] + accumulator2[1])\n",
        " \n",
        "comb_rdd = student_rdd.map(lambda t: (t[0], (t[1], t[2]))).combineByKey(createCombiner, mergeValue, mergeCombiner).map(lambda t: (t[0], t[1][0]/t[1][1]))\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I00lb6E9x9XH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b8867c-b293-49c5-b031-eff696bea10d"
      },
      "source": [
        "# Check the Outout\n",
        "for tpl in comb_rdd.collect():\n",
        "    print(tpl)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('suresh', 77.0)\n",
            "('ram', 82.5)\n",
            "('ramesh', 76.5)\n",
            "('Anu', 64.0)\n",
            "('john', 86.25)\n",
            "('sita', 65.0)\n",
            "('harsha', 76.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj-cIzcwzwhM"
      },
      "source": [
        "**mapValues:** mapValues function does not change the key. It applies a function to each value of a paired RDD of spark and it retains the partitioning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksEKVyfm0V0u"
      },
      "source": [
        "1. Find the number of programming languages that a person know in the below defined data by using the mapValues transformation.\n",
        "\n",
        "  data = [ ( \" bharat \", [ \" c \" , \" java \" , \" python \"]),\n",
        "  \n",
        "  ( \"chandu\",[ \" c \" , \" java \" , \" python \" , \" scala \" ] ) ,\n",
        "  \n",
        "  ( \"akash\", [\" c \" , \" java \" , \" python \" , \" scala \" , \" spark \" , \" cobal \" ] ) ,\n",
        "  \n",
        "  ( \" deepak \" , [ \" c \" , \" java \" , \"python \" ] ) ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GapW25Q70urA"
      },
      "source": [
        "student_language_rdd = sc.parallelize([(\"bharat\", [\"c\", \"java\", \"python\"]),(\"chandu\",[\"c\", \"java\", \"python\",\"scala\"]),(\"akash\", [\"c\", \"java\", \"python\",\"scala\",\"spark\",\"cobal\"]),(\"deepak\",[\"c\", \"java\", \"python\"])])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEfQqRLn1hr7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89eb3677-9dbc-4f23-a1aa-3b87a4957584"
      },
      "source": [
        "def f(student_language_rdd):\n",
        "   return len(student_language_rdd)\n",
        "resultant = student_language_rdd.mapValues(f)\n",
        "resultant.collect()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bharat', 3), ('chandu', 4), ('akash', 6), ('deepak', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSPXU0H_6IAg"
      },
      "source": [
        "**countByValue:** It count's the number of elements for each value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cz4kHRs6TG0"
      },
      "source": [
        "1. For the below defined data count the value(Using countByValue action) for the elements.\n",
        "\n",
        "  data = [1,1,1,2,2,3,3,1,1,2,3,3,2,2,3,3]\n",
        "    \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdJp1dZ46Xec"
      },
      "source": [
        "data = [1,1,1,2,2,3,3,1,1,2,3,3,2,2,3,3]\n",
        "data = sc.parallelize(data)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgb7c19c6Y-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ae8c79-cea4-472d-9837-a8c19d21d0e5"
      },
      "source": [
        "data.countByValue()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {1: 5, 2: 5, 3: 6})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7jPZhYoStqi"
      },
      "source": [
        "**Cartesian product:** The Cartesian function generates a Cartesian product of two RDDs and returns all the possible combination of pairs. Here, each element of one RDD is paired with each element of another dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YM3GQ1SS7Jr"
      },
      "source": [
        "1.Find the cartesian product of the given below data.\n",
        "\n",
        "data1 = [ \" spark \" , \" java \" , \" python \" , \" scala \" , \" c \"]\n",
        "\n",
        "data2 = [ \" Hive \" , \" kafka \" , \" H-base \" ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK8lh2K6Thzi"
      },
      "source": [
        "data1 = [\"spark\",\"java\",\"python\",\"scala\",\"c\"]\n",
        "data2 = [\"Hive\",\"kafka\",\"H-base\"]\n",
        "data1 = sc.parallelize(data1)\n",
        "data2 = sc.parallelize(data2)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dflPpieBTrK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a581daa7-9982-4088-948e-90e32ee1d762"
      },
      "source": [
        "data1.cartesian(data2).collect()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark', 'Hive'),\n",
              " ('java', 'Hive'),\n",
              " ('spark', 'kafka'),\n",
              " ('java', 'kafka'),\n",
              " ('spark', 'H-base'),\n",
              " ('java', 'H-base'),\n",
              " ('python', 'Hive'),\n",
              " ('scala', 'Hive'),\n",
              " ('c', 'Hive'),\n",
              " ('python', 'kafka'),\n",
              " ('scala', 'kafka'),\n",
              " ('python', 'H-base'),\n",
              " ('scala', 'H-base'),\n",
              " ('c', 'kafka'),\n",
              " ('c', 'H-base')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54bhb1dUdt3A"
      },
      "source": [
        "2.Implement cartesian product on the dataset defined below\n",
        "\n",
        "x = { ' A ' ,  2 , ' C '}\n",
        "\n",
        "y = { ' D ' , 1 }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVuZnfBQdmJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b3a6702-9111-44a2-e27c-dbeaecb05329"
      },
      "source": [
        "x = sc.parallelize({'A', 2, 'C'})\n",
        "y = sc.parallelize({'D', 1})\n",
        "x.cartesian(y).collect() "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A', 1), ('A', 'D'), (2, 1), ('C', 1), (2, 'D'), ('C', 'D')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqzBtyHLUMlM"
      },
      "source": [
        "**distinct:** Return a new RDD containing the distinct elements in this RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTF8GlWFURwa"
      },
      "source": [
        "1. Find the distinct elements from the below given data.\n",
        "\n",
        "  data = [ 1 , 1 , 2 , 2 , 3 , 6 , 0 , 5 , 4 , 5 , 6 , 8 , 5 , 3 , 5 , 7 , 4 , 2 , 4 , 6 , 0 ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpK9UbuJUqP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d49d98-82b3-48b4-989b-3133374ddd43"
      },
      "source": [
        "sc.parallelize([ 1, 1, 2, 2, 3, 6, 0, 5, 4, 5, 6, 8, 5, 3, 5, 7, 4, 2, 4, 6, 0, 4]).distinct().collect()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 6, 0, 4, 8, 1, 3, 5, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0EqUd79tA0x"
      },
      "source": [
        "2. FInd the distint elements from the below defined data.\n",
        "\n",
        "  data = [ \" c \" , \" c++ \" ,  \" c# \", \" java \" , \" python \" , \" scala \" , \" c \" , \" java \" , \" python \" , \" scala \" , \" spark \" , \" cobal \" , \" c++ \" , \" c# \" ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeENBGIhtlY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ecf0e3-0d81-42dd-f060-526ed47ebb00"
      },
      "source": [
        "data = [\"c\", \"c++\", \"c#\", \"java\", \"python\", \"scala\", \"c\", \"java\", \"python\", \"scala\", \"spark\", \"cobal\", \"c++\", \"c#\"]\n",
        "sc.parallelize(data).distinct().collect()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['c', 'c++', 'java', 'python', 'scala', 'c#', 'spark', 'cobal']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0fE8R6Ud-s8"
      },
      "source": [
        "#@title Q.1. What is the function we use when we come across disk spilling problem while handling the data?  { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer1 = \"\" #@param [\"\",\"repartition()\", \"filter()\",\"map()\", \"All the above\"]\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlSp1X_GeF41"
      },
      "source": [
        "#@title Q.2.  Statement: We can save intermediate results only to Disk using Persist.(True or False){ run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer2 = \"\" #@param [\"\",\"False\", \"True\"]\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67821af5-4344-4915-e727-f448dfa0244b"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please answer Question 1\n"
          ]
        }
      ]
    }
  ]
}